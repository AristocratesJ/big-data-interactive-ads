services:
  # --- 1. ZOOKEEPER (Wymagany przez KafkÄ™ i HBase) ---
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    mem_limit: 512m
    networks:
      - bigdata-network

  # --- 2. KAFKA (Buffering Layer) ---
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_HEAP_OPTS: "-Xmx512m -Xms512m"
    mem_limit: 1g
    networks:
      - bigdata-network
    healthcheck:
      test:
        [
          "CMD",
          "kafka-broker-api-versions",
          "--bootstrap-server=localhost:9092",
        ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s

  # --- 3. KAFKA UI (Monitoring) ---
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8090:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:29092
      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181
    depends_on:
      - kafka
      - zookeeper
    mem_limit: 512m
    networks:
      - bigdata-network

  # --- 4. APACHE NIFI (Ingestion Layer) ---
  nifi:
    image: apache/nifi:1.23.2
    container_name: nifi
    ports:
      - "8443:8443"
    environment:
      - SINGLE_USER_CREDENTIALS_USERNAME=admin
      - SINGLE_USER_CREDENTIALS_PASSWORD=adminadmin123
    volumes:
      - nifi_conf:/opt/nifi/nifi-current/conf
      - nifi_database_repository:/opt/nifi/nifi-current/database_repository
      - nifi_flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
      - nifi_content_repository:/opt/nifi/nifi-current/content_repository
      - nifi_provenance_repository:/opt/nifi/nifi-current/provenance_repository
      - nifi_state:/opt/nifi/nifi-current/state
      - nifi_logs:/opt/nifi/nifi-current/logs
    mem_limit: 2g
    networks:
      - bigdata-network
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "grep -q 'Started Application Controller' /opt/nifi/nifi-current/logs/nifi-app.log 2>/dev/null || exit 1",
        ]
      interval: 15s
      timeout: 5s
      retries: 15
      start_period: 90s

  # --- 5. SPARK MASTER (Processing Layer) ---
  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MODE=master
      - SPARK_LOCAL_DIRS=/tmp
      - HOME=/home/spark
      - TZ=Europe/Warsaw
      - SPARK_DAEMON_JAVA_OPTS=-Duser.timezone=Europe/Warsaw
    volumes:
      - ./spark:/opt/spark-apps # Mount your scripts
      - spark_ivy:/home/spark/.ivy2 # Cache packages
      - ./logs:/opt/logs # Centralized logs
    ports:
      - "8080:8080"
      - "7077:7077"
    mem_limit: 3.5g
    networks:
      - bigdata-network

  # --- 6. SPARK WORKER ---
  spark-worker:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=10
      - HOME=/home/spark
      - TZ=Europe/Warsaw
    depends_on:
      - spark-master
    volumes:
      - ./spark:/opt/spark-apps
    mem_limit: 3.5g
    networks:
      - bigdata-network

  # --- 7. HADOOP NAMENODE (HDFS Storage Layer) ---
  namenode:
    image: bde2020/hadoop-namenode:latest
    container_name: namenode
    environment:
      - CLUSTER_NAME=bigdata
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    mem_limit: 512m
    networks:
      - bigdata-network

  # --- 8. HADOOP DATANODE ---
  datanode:
    image: bde2020/hadoop-datanode:latest
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    depends_on:
      - namenode
    mem_limit: 1g
    networks:
      - bigdata-network

  # --- 9. HBASE (Standalone with embedded RegionServer) ---
  hbase:
    image: harisekhon/hbase:latest
    container_name: hbase
    hostname: hbase
    ports:
      - "16010:16010" # HBase Master Web UI
      - "16000:16000" # HBase Master
      - "16020:16020" # HBase RegionServer
      - "16030:16030" # HBase RegionServer Info
      - "9090:9090" # HBase Thrift Server
      - "9095:9095" # HBase Thrift2 Server
      - "2888:2888" # HBase Zookeeper peer
      - "3888:3888" # HBase Zookeeper leader
    depends_on:
      - zookeeper
    mem_limit: 2g
    networks:
      - bigdata-network
    environment:
      - HBASE_MANAGES_ZK=false
    healthcheck:
      test: ["CMD-SHELL", "echo 'status' | hbase shell -n | grep -q 'active'"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 90s

  # --- 10. AUTO-SETUP SERVICE (Runs once after all services are healthy) ---
  setup:
    image: python:3.12-slim
    container_name: bigdata-setup
    depends_on:
      kafka:
        condition: service_healthy
      hbase:
        condition: service_healthy
      nifi:
        condition: service_healthy
    volumes:
      - .:/app
    working_dir: /app
    env_file:
      - .env
    networks:
      - bigdata-network
    command: >
      bash -c "
        echo '===== Installing Python dependencies =====' &&
        pip install --quiet --root-user-action=ignore kafka-python happybase requests urllib3 &&
        echo '===== Running automated setup =====' &&
        python scripts/setup_all.py &&
        echo '===== Setup complete! ====='
      "
    restart: "no"

networks:
  bigdata-network:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode:
  nifi_conf:
  nifi_database_repository:
  nifi_flowfile_repository:
  nifi_content_repository:
  nifi_provenance_repository:
  nifi_state:
  nifi_logs:
  spark_ivy:
