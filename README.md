# Big Data - Interactive Advertising Analytics Project

## ğŸ¯ Cel projektu

Stworzenie systemu do analizy pogody, ruchu drogowego i sentymentu w Warszawie w celu wykorzystania w reklamamach. Projekt powinen umoÅ¼liwiaÄ‡:

- Wykrywanie korkÃ³w
- AnalizÄ™ skutecznoÅ›ci pogody
- AnalizÄ™ tweetÃ³w pod kÄ…tem negatywnego sentymentu
- CaÅ‚oÅ›ciowe wykorzystanie analiz w celu wybrania miejsca i czasu na wyÅ›wietlanie reklam

---

## ğŸ› ï¸ Stos technologiczny

| Technologia      | Wersja | Zastosowanie                         | Port       |
| ---------------- | ------ | ------------------------------------ | ---------- |
| **Apache Kafka** | 7.4.0  | Buforowanie i streaming danych       | 9092       |
| **Kafka UI**     | latest | Interfejs do monitorowania Kafka     | 8090       |
| **Apache NiFi**  | latest | Pobieranie i preprocessing danych    | 8443       |
| **Apache Spark** | latest | Przetwarzanie danych i analityka     | 8080, 7077 |
| **Hadoop HDFS**  | latest | DÅ‚ugoterminowe przechowywanie danych | 9870, 9000 |
| **HBase**        | latest | Szybki dostÄ™p do danych              | 16010      |
| **Zookeeper**    | 7.4.0  | Koordynacja usÅ‚ug rozproszonych      | 2181       |

---

## ğŸ“¦ Instalacja

### Wymagania systemowe

- **System operacyjny**: Windows 10/11 z WSL 2 (lub Linux/macOS)
- **RAM**: 16GB
- **Dysk**: ~10GB wolnego miejsca
- **Docker Desktop**: najnowsza wersja

### Instalacja na Windows

#### 1. Instalacja WSL 2

OtwÃ³rz PowerShell jako **Administrator** i wykonaj:

```powershell
wsl --install
```

Po instalacji system poprosi o restart. Po restarcie:

- Ustaw login i hasÅ‚o dla WSL (nie bÄ™dzie to potem potrzebne, ale proponujÄ™ jakieÅ› Å‚atwe typu admin, password)

#### 2. Instalacja Docker Desktop

1. Pobierz Docker Desktop ze strony: https://www.docker.com/products/docker-desktop/
2. Uruchom instalator
3. **WAÅ»NE**: Podczas instalacji upewnij siÄ™, Å¼e zaznaczone jest:
   - âœ… **Use WSL 2 based engine**
4. Po instalacji uruchom Docker Desktop
5. Poczekaj aÅ¼ Docker siÄ™ w peÅ‚ni uruchomi

#### 3. Uruchomienie projektu

Sklonuj repozytorium i przejdÅº do katalogu projektu:

```powershell
cd big-data-interactive-ads
```

StwÃ³rz Å›rodowisko wirtualne i zaimportuj biblioteki python za pomocÄ… uv:

```powershell
uv venv .venv
.venv\Scripts\activate
uv sync
```

JeÅ›li nie posiadasz uv:

```powershell
pip install uv
```

Uruchom wszystkie usÅ‚ugi na docker:

```powershell
docker-compose up -d
```

#### 4. Weryfikacja instalacji

Poczekaj 2-3 minuty na uruchomienie wszystkich usÅ‚ug, nastÄ™pnie sprawdÅº status:

```powershell
docker-compose ps
```

**PrawidÅ‚owy wynik powinien wyglÄ…daÄ‡ tak:**

```
NAME           IMAGE                             STATUS
datanode       bde2020/hadoop-datanode:latest    Up
hbase          harisekhon/hbase:latest           Up
kafka          confluentinc/cp-kafka:7.4.0       Up
kafka-ui       provectuslabs/kafka-ui:latest     Up
namenode       bde2020/hadoop-namenode:latest    Up
nifi           apache/nifi:latest                Up
spark-master   apache/spark:latest               Up
spark-worker   apache/spark:latest               Up
zookeeper      confluentinc/cp-zookeeper:7.4.0   Up
```

Wszystkie kontenery powinny mieÄ‡ status **"Up"**.

#### 5. DostÄ™p do interfejsÃ³w webowych

Po uruchomieniu, sprawdÅº czy wszystkie interfejsy sÄ… dostÄ™pne:

| UsÅ‚uga              | URL                         | Opis                                                       |
| ------------------- | --------------------------- | ---------------------------------------------------------- |
| **Kafka UI**        | http://localhost:8090       | Monitor Kafka topics i messages                            |
| **NiFi**            | https://localhost:8443/nifi | PrzepÅ‚ywy danych (login: `admin` / hasÅ‚o: `adminadmin123`) |
| **Spark Master**    | http://localhost:8080       | Monitor Spark jobs                                         |
| **Hadoop NameNode** | http://localhost:9870       | HDFS filesystem                                            |
| **HBase Master**    | http://localhost:16010      | HBase tables                                               |

---

#### 6. Kafka i HBase

StwÃ³rz tabele HBase i tematy Kafka:

```powershell
.\scripts\current_setup_windows.ps1
```

#### 7. Spark

Rozpocznij zadania ze Spark

```powershell
.\scripts\run_spark_jobs.ps1
```

Teraz sprawdÅº czy zadania na Spark'u siÄ™ odpaliÅ‚y http://localhost:8080. Poczekaj 30 sek. JeÅ›li koÅ„cowo nie zobaczysz wszystkich 4 odpalonych zadaÅ„ spark (sekcja Running Applications) wykonaj:

```powershell
.\scripts\stop_spark_jobs.ps1
.\scripts\run_spark_jobs.ps1
```

PowinieneÅ› wkrÃ³tce zobaczyÄ‡ wszystkie zadania aktywne. JeÅ›li nie skontaktuj siÄ™ z Barteczkiem.

#### 8. Testuj

Zaimportuj template nifi i uruchom. Poczekaj chwile. NastÄ™pnie uruchom shell HBase:

```powershell
docker-compose exec hbase hbase shell
```

SprawdÅº czy dane zostajÄ… wrzucone do tabeli:

```hbase
list
scan 'transport_events', {LIMIT => 1}
scan 'air_quality_forecast', {LIMIT => 1}
scan 'weather_forecast', {LIMIT => 1}
```

## âš™ï¸ Uruchamianie wybranych usÅ‚ug

Ze wzglÄ™du na ograniczenia pamiÄ™ci RAM (16GB), moÅ¼esz uruchamiaÄ‡ tylko wybrane usÅ‚ugi zamiast caÅ‚ego stosu.

### PrzykÅ‚ad: Core Services (Kafka + Zookeeper)

Tylko podstawowe usÅ‚ugi do przesyÅ‚ania danych:

```powershell
docker-compose up -d zookeeper kafka kafka-ui
```

### Zatrzymywanie usÅ‚ug

Zatrzymaj wszystkie uruchomione usÅ‚ugi:

```powershell
docker-compose down
```

Zatrzymaj wybrane usÅ‚ugi (np. tylko NiFi):

```powershell
docker-compose stop nifi
```

### Sprawdzanie uÅ¼ycia zasobÃ³w

Monitoruj uÅ¼ycie pamiÄ™ci RAM i CPU przez kontenery:

```powershell
docker stats --no-stream
```
