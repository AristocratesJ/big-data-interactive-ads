# Big Data - Interactive Advertising Analytics Project

## üéØ Cel projektu

Stworzenie systemu do analizy pogody, ruchu drogowego i sentymentu w Warszawie w celu wykorzystania w reklamamach. Projekt powinen umo≈ºliwiaƒá:

- Wykrywanie kork√≥w
- Analizƒô skuteczno≈õci pogody
- Analizƒô tweet√≥w pod kƒÖtem negatywnego sentymentu
- **Decyzje Reklamowe (Ad Decision Engine)**: Automatyczne podejmowanie decyzji o wy≈õwietleniu reklamy "na pocieszenie" (kampania eskapistyczna) w oparciu o z≈ÇƒÖ pogodƒô, korki i negatywne nastroje spo≈Çeczne.

---

## üõ†Ô∏è Stos technologiczny

| Technologia      | Wersja   | Zastosowanie                         | Port       |
| ---------------- | -------- | ------------------------------------ | ---------- |
| **Apache Kafka** | 7.4.0    | Buforowanie i streaming danych       | 9092       |
| **Kafka UI**     | latest   | Interfejs do monitorowania Kafka     | 8090       |
| **Apache NiFi**  | latest   | Pobieranie i preprocessing danych    | 8443       |
| **Apache Spark** | latest   | Przetwarzanie danych i analityka     | 8080, 7077 |
| **Hadoop HDFS**  | latest   | D≈Çugoterminowe przechowywanie danych | 9870, 9000 |
| **HBase**        | latest   | Szybki dostƒôp do danych              | 16010      |
| **Apache Hive**  | Embedded | Hurtownia danych (via Spark)         | -          |
| **Zookeeper**    | 7.4.0    | Koordynacja us≈Çug rozproszonych      | 2181       |

---

## ÔøΩ Konfiguracja Twitter API

Aby pobieraƒá tweety z Warszawy, musisz skonfigurowaƒá klucz API Twittera.

### 1. Utw√≥rz plik `.env`

W g≈Ç√≥wnym katalogu projektu utw√≥rz plik `.env` z kluczem API:

```env
TWITTER_API_KEY=tw√≥j_klucz_api_tutaj
```

### 2. SkƒÖd wziƒÖƒá klucz API?

Projekt u≈ºywa zewnƒôtrznego API Twitter (`api.twitterapi.io`). Aby uzyskaƒá klucz:

1. Zarejestruj siƒô na platformie dostawcy API
2. Wygeneruj klucz API
3. Skopiuj klucz do pliku `.env`

### 3. Weryfikacja konfiguracji

Po uruchomieniu systemu, NiFi automatycznie u≈ºyje klucza z pliku `.env` do uwierzytelniania ≈ºƒÖda≈Ñ do Twitter API.

Mo≈ºesz przetestowaƒá po≈ÇƒÖczenie:

```powershell
python tests/twitter_api.py
```

> **Uwaga**: Bez prawid≈Çowego klucza API, pobieranie tweet√≥w nie bƒôdzie dzia≈Çaƒá, ale pozosta≈Çe ≈∫r√≥d≈Ça danych (ZTM, pogoda, jako≈õƒá powietrza) bƒôdƒÖ funkcjonowaƒá normalnie.

---

## ÔøΩüì¶ Instalacja

### Wymagania systemowe

- **System operacyjny**: Windows 10/11 z WSL 2, Linux lub macOS
- **RAM**: 16GB
- **Dysk**: ~10GB wolnego miejsca
- **Docker Desktop**: najnowsza wersja

### Instalacja na Windows

#### 1. Instalacja WSL 2

Otw√≥rz PowerShell jako **Administrator** i wykonaj:

```powershell
wsl --install
```

Po instalacji system poprosi o restart. Po restarcie:

- Ustaw login i has≈Ço dla WSL (nie bƒôdzie to potem potrzebne, ale proponujƒô jakie≈õ ≈Çatwe typu admin, password)

#### 2. Instalacja Docker Desktop

1. Pobierz Docker Desktop ze strony: https://www.docker.com/products/docker-desktop/
2. Uruchom instalator
3. **WA≈ªNE**: Podczas instalacji upewnij siƒô, ≈ºe zaznaczone jest:
   - ‚úÖ **Use WSL 2 based engine**
4. Po instalacji uruchom Docker Desktop
5. Poczekaj a≈º Docker siƒô w pe≈Çni uruchomi

#### 3. Uruchomienie projektu

Sklonuj repozytorium i przejd≈∫ do katalogu projektu:

```powershell
cd big-data-interactive-ads
```

Stw√≥rz ≈õrodowisko wirtualne i zaimportuj biblioteki python za pomocƒÖ uv:

```powershell
uv venv .venv
.venv\Scripts\activate
uv sync
```

Je≈õli nie posiadasz uv:

```powershell
pip install uv
```

Uruchom wszystkie us≈Çugi na docker:

```powershell
docker-compose up -d
```

#### 4. Weryfikacja instalacji

Poczekaj 2-3 minuty na uruchomienie wszystkich us≈Çug, nastƒôpnie sprawd≈∫ status:

```powershell
docker-compose ps
```

**Prawid≈Çowy wynik powinien wyglƒÖdaƒá tak:**

```
NAME           IMAGE                             STATUS
datanode       bde2020/hadoop-datanode:latest    Up
hbase          harisekhon/hbase:latest           Up
kafka          confluentinc/cp-kafka:7.4.0       Up
kafka-ui       provectuslabs/kafka-ui:latest     Up
namenode       bde2020/hadoop-namenode:latest    Up
nifi           apache/nifi:latest                Up
spark-master   apache/spark:latest               Up
spark-worker   apache/spark:latest               Up
zookeeper      confluentinc/cp-zookeeper:7.4.0   Up
```

Wszystkie kontenery powinny mieƒá status **"Up"**.

#### 5. Dostƒôp do interfejs√≥w webowych

Po uruchomieniu, sprawd≈∫ czy wszystkie interfejsy sƒÖ dostƒôpne:

| Us≈Çuga              | URL                         | Opis                                                       |
| ------------------- | --------------------------- | ---------------------------------------------------------- |
| **Kafka UI**        | http://localhost:8090       | Monitor Kafka topics i messages                            |
| **NiFi**            | https://localhost:8443/nifi | Przep≈Çywy danych (login: `admin` / has≈Ço: `adminadmin123`) |
| **Spark Master**    | http://localhost:8080       | Monitor Spark jobs                                         |
| **Hadoop NameNode** | http://localhost:9870       | HDFS filesystem                                            |
| **HBase Master**    | http://localhost:16010      | HBase tables                                               |

---

#### 6. Weryfikacja automatycznej konfiguracji

Po uruchomieniu `docker-compose up -d`, system **automatycznie** wykonuje pe≈ÇnƒÖ konfiguracjƒô:

- ‚úÖ Czeka na gotowo≈õƒá wszystkich us≈Çug (Kafka, HBase, NiFi)
- ‚úÖ Tworzy wszystkie tematy Kafka
- ‚úÖ Tworzy wszystkie tabele HBase
- ‚úÖ Wgrywa i instancjonuje szablon NiFi na canvas

Sprawd≈∫ logi automatycznej konfiguracji:

```powershell
docker-compose logs setup
```

Na ko≈Ñcu log√≥w powiniene≈õ zobaczyƒá:

```
‚úì SETUP COMPLETED SUCCESSFULLY!
```

Je≈õli zobaczysz b≈Çƒôdy, uruchom ponownie:

```powershell
docker-compose restart setup
docker-compose logs -f setup
```

#### 7. Uruchomienie przep≈Çyw√≥w danych

> **Wa≈ºne**: Po automatycznej konfiguracji z poprzedniego kroku, musisz rƒôcznie uruchomiƒá przep≈Çywy danych. Automatyczna konfiguracja tylko **przygotowuje** infrastrukturƒô (tematy, tabele, szablon), ale nie startuje pobierania i przetwarzania danych.

**7.1. Uruchom przep≈Çywy NiFi** (pobieranie danych z API):

#### macOS / Linux

```bash
./scripts/run_nifi_flows.sh
```

#### Windows (PowerShell)

```powershell
.\scripts\run_nifi_flows.ps1
```

To uruchomi wszystkie procesory NiFi, kt√≥re bƒôdƒÖ pobieraƒá dane z:

- ZTM API (autobusy i trolejbusy)
- Open-Meteo API (pogoda i jako≈õƒá powietrza)
- Twitter API (tweety z Warszawy)

**7.2. Uruchom zadania Spark** (przetwarzanie danych):

#### macOS / Linux

```bash
./scripts/run_spark_jobs.sh
```

#### Windows (PowerShell)

```powershell
.\scripts\run_spark_jobs.ps1
```

To uruchomi:

1. **5 zada≈Ñ Spark Streaming** (przetwarzanie danych z Kafka do HBase):
   - Buses
   - Trolleys
   - Weather
   - Air Quality
   - Twitter Sentiment
2. **Ad Campaign Manager** (niezale≈ºny proces Python podejmujƒÖcy decyzje)
3. **Hive Archiver Scheduler** (automatyczny proces w tle)

Teraz sprawd≈∫ czy zadania na Spark'u siƒô odpali≈Çy: http://localhost:8080

Poczekaj 30 sek. Powiniene≈õ zobaczyƒá **5 aktywnych aplikacji streamingowych** w sekcji "Running Applications".

> **Uwaga**: `ad_campaign_manager.py` oraz `archive_to_hive.py` (scheduler) dzia≈ÇajƒÖ jako procesy w tle i nie zawsze sƒÖ widoczne na g≈Ç√≥wnej li≈õcie aplikacji streamingowych w Spark UI (chyba ≈ºe w momencie wykonywania batcha).

Je≈õli nie zobaczysz zada≈Ñ, zrestartuj:

#### macOS / Linux

```bash
./scripts/stop_spark_jobs.sh
./scripts/run_spark_jobs.sh
```

#### Windows (PowerShell)

```powershell
.\scripts\stop_spark_jobs.ps1
.\scripts\run_spark_jobs.ps1
```

**Zatrzymywanie przep≈Çyw√≥w danych:**

#### macOS / Linux

```bash
# Zatrzymaj NiFi procesory
./scripts/stop_nifi_flows.sh

# Zatrzymaj zadania Spark + Ad Manager
./scripts/stop_spark_jobs.sh
```

#### Windows (PowerShell)

```powershell
# Zatrzymaj NiFi procesory
.\scripts\stop_nifi_flows.ps1

# Zatrzymaj zadania Spark + Ad Manager
.\scripts\stop_spark_jobs.ps1
```

#### 8. Weryfikacja dzia≈Çania systemu

Poczekaj 2-3 minuty na zebranie pierwszych danych, nastƒôpnie zweryfikuj:

**8.1. Sprawd≈∫ dane w HBase:**

```powershell
docker-compose exec hbase hbase shell
```

W HBase shell wykonaj:

```hbase
list
scan 'transport_events', {LIMIT => 1}
scan 'air_quality_forecast', {LIMIT => 1}
scan 'weather_forecast', {LIMIT => 1}
scan 'tweets', {LIMIT => 1}
scan 'ad_decisions', {LIMIT => 1}
exit
```

Je≈õli zobaczysz dane w tabelach - system dzia≈Ça poprawnie! ‚úÖ

**8.2. Monitoruj dane w Kafka UI:**

Otw√≥rz http://localhost:8090 i sprawd≈∫ tematy:

- `ztm-buses-raw` - powinny pojawiaƒá siƒô dane o autobusach
- `weather-forecast-raw` - dane pogodowe
- `air-quality-raw` - dane o jako≈õci powietrza
- `tweets-warsaw-raw` - tweety z Warszawy
- `ad-decisions` - wyniki decyzji reklamowych

---

## üìä Analityka i Archiwizacja (Hive)

System posiada dedykowanƒÖ warstwƒô analitycznƒÖ opartƒÖ o **Apache Hive** (zintegrowany ze Spark SQL), kt√≥ra archiwizuje decyzje reklamowe na HDFS w formacie Parquet z partycjonowaniem Hive.

### 1. Architektura

- **Decyzje (Real-time)**: `ad_campaign_manager.py` (Docker Service) wysy≈Ça decyzje do **Kafka** (`ad-decisions`) i **HBase** co minutƒô.
- **Archivizacja (Batch)**: `archive_scheduler.py` (Docker Service) uruchamia co godzinƒô przenoszenie danych z HBase do tabeli Hive na HDFS w lokalizacji `/user/archive/ad_decisions` (format Parquet, partycjonowanie po dacie i godzinie).

### 2. Monitorowanie Archiwizacji

Archive Scheduler dzia≈Ça automatycznie jako Docker service. Sprawd≈∫ jego status:

#### macOS / Linux / Windows

```bash
# Sprawd≈∫ logi schedulera
docker logs -f archive-scheduler

# Lub sprawd≈∫ szczeg√≥≈Çowe logi job√≥w
docker exec archive-scheduler tail -f /opt/spark-apps/scheduler.log
```

---

## üîç Jak Zapytaƒá Wyniki?

System oferuje piƒôƒá sposob√≥w dostƒôpu do danych: **HBase** (real-time), **Hive/HDFS** (archiwum), **Jupyter Notebook** (analiza wizualna), **HDFS Browser** i **Kafka** (streaming).

### Metoda 1: Zapytania HBase (Real-Time Data)

HBase przechowuje dane z ostatnich 24 godzin - najlepsze dla zapyta≈Ñ real-time.

#### Przyk≈Çad 1: Skanowanie ostatnich decyzji

```bash
# Uruchom HBase shell
docker exec -it hbase hbase shell

# W HBase shell:
# Sprawd≈∫ ostatnie 5 decyzji
scan 'ad_decisions', {LIMIT => 5}

# Sprawd≈∫ konkretnƒÖ decyzjƒô
get 'ad_decisions', '20260108_143000'

# Skanuj zakres czasowy (ostatnie 10 minut)
scan 'ad_decisions', {STARTROW => '20260108_143000', STOPROW => '20260108_144000'}

# Wyjd≈∫
exit
```

#### Przyk≈Çad 2: Sprawdzanie innych tabel

```bash
# Transport events
scan 'transport_events', {LIMIT => 2}

# Weather forecast
scan 'weather_forecast', {LIMIT => 2}

# Air quality
scan 'air_quality_forecast', {LIMIT => 2}

# Tweets
scan 'tweets', {LIMIT => 2}
```

---

### Metoda 2: Zapytania Hive/Spark SQL (Historical Data)

Dane archiwalne w formacie Parquet mo≈ºna odpytywaƒá przez Spark SQL. 

> **Wa≈ºne**: Zapytania muszƒÖ byƒá wykonywane przez kontener `archive-scheduler`, kt√≥ry ma skonfigurowany dostƒôp do Hive metastore.

```bash
# Uruchom Spark SQL shell
docker exec -it archive-scheduler /opt/spark/bin/spark-sql

# W Spark SQL:
# Poka≈º wszystkie tabele
SHOW TABLES;

# Sprawd≈∫ schemat tabeli
DESCRIBE ad_decisions_archive;

# Sprawd≈∫ partycje
SHOW PARTITIONS ad_decisions_archive;

# Prosty SELECT
SELECT * FROM ad_decisions_archive LIMIT 10;

# Statystyki decyzji
SELECT 
    decision_result, 
    COUNT(*) as count,
    AVG(global_score) as avg_score
FROM ad_decisions_archive
GROUP BY decision_result;

# Decyzje z konkretnej daty
SELECT * FROM ad_decisions_archive 
WHERE dt = '20260108' 
LIMIT 20;

# Decyzje z konkretnej godziny
SELECT * FROM ad_decisions_archive 
WHERE dt = '20260108' AND hr = '14'
ORDER BY decision_id DESC;

# Top 10 najwy≈ºszych score'√≥w
SELECT decision_id, global_score, decision_result 
FROM ad_decisions_archive 
ORDER BY global_score DESC 
LIMIT 10;
```

---

### Metoda 3: Analiza w Jupyter Notebook

System zawiera gotowy Jupyter Notebook z wizualizacjami.

#### Uruchomienie Notebook

1. **Zainstaluj Jupyter** (je≈õli nie masz):
   ```bash
   pip install jupyter pandas matplotlib seaborn happybase
   ```

2. **Uruchom Jupyter**:
   ```bash
   jupyter notebook
   ```

3. **Otw√≥rz** `analyse/ad_decisions_analysis.ipynb`

#### Co znajdziesz w Notebook?

- üìà **Wykres czasowy** global score
- üìä **Rozk≈Çad decyzji** (SHOW_AD vs NO_AD)
- üî• **Heatmapa korelacji** miƒôdzy wska≈∫nikami
- üìâ **Statystyki opisowe** dla wszystkich score'√≥w
- üïê **Analiza wzorc√≥w czasowych** (godziny szczytu)

#### Przyk≈Çad u≈ºycia Notebook:

```python
# Load last 24 hours of data from HBase
decisions = load_decisions_from_hbase(hours=24)
df = pd.DataFrame(decisions)

# Quick stats
print(df.describe())

# Decision distribution
df['decision'].value_counts().plot(kind='bar')

# Score correlation heatmap
sns.heatmap(df[['traffic_score', 'weather_score', 'sentiment_score', 'global_score']].corr(), 
            annot=True, cmap='coolwarm')
```

---

### Metoda 4: PrzeglƒÖdanie HDFS przez PrzeglƒÖdarkƒô

**URL**: http://localhost:9870/explorer.html#/user/archive/ad_decisions

Mo≈ºesz:
- PrzeglƒÖdaƒá strukturƒô partycji (`dt=20260108/hr=14/`)
- Pobieraƒá pliki Parquet
- Sprawdzaƒá rozmiar danych
- Weryfikowaƒá uprawnienia

---

### Metoda 5: Monitoring Kafka (Real-Time Decisions)

Monitoruj decyzje w czasie rzeczywistym przez Kafka UI lub bezpo≈õrednio z topicu.

#### Kafka UI

**URL**: http://localhost:8090

1. Przejd≈∫ do **Topics** ‚Üí **ad-decisions**
2. Kliknij **Messages**
3. Zobacz ostatnie decyzje w czasie rzeczywistym

#### Konsumowanie przez CLI

```bash
docker exec -it kafka kafka-console-consumer \
    --bootstrap-server localhost:9092 \
    --topic ad-decisions \
    --from-beginning \
    --max-messages 10
```

---

## üß™ Przyk≈Çadowe Zapytania Analityczne

### 1. Skuteczno≈õƒá kampanii (conversion rate)

```sql
-- W Spark SQL
SELECT 
    dt,
    COUNT(*) as total,
    SUM(CASE WHEN decision_result = 'SHOW_AD' THEN 1 ELSE 0 END) as shown,
    ROUND(SUM(CASE WHEN decision_result = 'SHOW_AD' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as show_rate_pct
FROM ad_decisions_archive
GROUP BY dt
ORDER BY dt DESC;
```

### 2. ≈örednie score'y w szczycie vs poza szczytem

```sql
SELECT 
    CASE 
        WHEN hr IN ('07','08','09','16','17','18') THEN 'Rush Hour'
        ELSE 'Off-Peak'
    END as period,
    AVG(traffic_score) as avg_traffic,
    AVG(weather_score) as avg_weather,
    AVG(global_score) as avg_global
FROM ad_decisions_archive
GROUP BY 
    CASE 
        WHEN hr IN ('07','08','09','16','17','18') THEN 'Rush Hour'
        ELSE 'Off-Peak'
    END;
```

### 3. Top 5 dni z najwy≈ºszym global score

```sql
SELECT 
    dt,
    AVG(global_score) as avg_score,
    MAX(global_score) as max_score,
    COUNT(*) as decisions
FROM ad_decisions_archive
GROUP BY dt
ORDER BY avg_score DESC
LIMIT 5;
```

---

## üìã Szybkie Por√≥wnanie Metod Dostƒôpu

| Metoda | Zakres Danych | Op√≥≈∫nienie | Najlepsze Dla |
|--------|---------------|------------|---------------|
| **HBase Shell** | Ostatnie 24h | <10ms | Real-time monitoring, pojedyncze rekordy |
| **Spark SQL/Hive** | Pe≈Çne archiwum | ~5-10s | Agregacje, analizy historyczne, raporty |
| **Jupyter Notebook** | Ostatnie 24h (HBase) | <1s | Wizualizacje, eksploracja danych |
| **Kafka UI** | Streaming | Real-time | Monitoring decyzji na ≈ºywo |
| **HDFS Browser** | Pe≈Çne archiwum | - | PrzeglƒÖdanie plik√≥w, weryfikacja partycji |

---

## üîÑ Podsumowanie workflow

```
1. docker-compose up -d          ‚Üí Uruchamia wszystkie us≈Çugi + auto-konfiguracja
2. docker-compose logs setup     ‚Üí Sprawd≈∫ czy konfiguracja siƒô powiod≈Ça
3. .\scripts\run_nifi_flows.ps1  ‚Üí Uruchom pobieranie danych
4. .\scripts\run_spark_jobs.ps1  ‚Üí Uruchom przetwarzanie danych
5. Monitoruj w UI                ‚Üí Kafka UI, NiFi, Spark Master, HBase
```

**Ponowne uruchomienie po zatrzymaniu:**

#### macOS / Linux

```bash
docker-compose down              # Zatrzymaj wszystko
docker-compose up -d             # Uruchom ponownie
./scripts/run_nifi_flows.sh      # Uruchom NiFi
./scripts/run_spark_jobs.sh      # Uruchom Spark
```

#### Windows (PowerShell)

```powershell
docker-compose down              # Zatrzymaj wszystko
docker-compose up -d             # Uruchom ponownie
.\scripts\run_nifi_flows.ps1     # Uruchom NiFi
.\scripts\run_spark_jobs.ps1     # Uruchom Spark
```

## ‚öôÔ∏è Uruchamianie wybranych us≈Çug

Ze wzglƒôdu na ograniczenia pamiƒôci RAM (16GB), mo≈ºesz uruchamiaƒá tylko wybrane us≈Çugi zamiast ca≈Çego stosu.

### Przyk≈Çad: Core Services (Kafka + Zookeeper)

Tylko podstawowe us≈Çugi do przesy≈Çania danych:

```powershell
docker-compose up -d zookeeper kafka kafka-ui
```

### Zatrzymywanie us≈Çug

Zatrzymaj wszystkie uruchomione us≈Çugi:

```powershell
docker-compose down
```

Zatrzymaj wybrane us≈Çugi (np. tylko NiFi):

```powershell
docker-compose stop nifi
```

### Sprawdzanie u≈ºycia zasob√≥w

Monitoruj u≈ºycie pamiƒôci RAM i CPU przez kontenery:

```powershell
docker stats --no-stream
```
